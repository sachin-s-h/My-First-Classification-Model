# -*- coding: utf-8 -*-
"""
Created on Thu Oct 24 12:33:55 2019

@author: Sachin
"""

from nltk.stem import WordNetLemmatizer
import re
import nltk
import json
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,SnowballStemmer
import os
import numpy as np
import pandas as pd
import random
from sklearn.base import TransformerMixin 
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer
from scipy.sparse import csr_matrix
from scipy import sparse
from sklearn.metrics import f1_score, accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.externals import joblib
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import train_test_split
from collections import Counter 
from nltk.corpus import stopwords
import pickle
import string
punctuation=string.punctuation

class predictors(TransformerMixin):
    def transform(self, X, **transform_params):
        df = pd.DataFrame()
        df['Sentence'] = X.values
        ret_res = text_transform(df)
        ret_res['most_sum']=df['Sentence'].apply(most_common)
        ret_res['most_by_word_ratio']=ret_res['most_sum']/ret_res['word_count']
        col = ['word_count', 'has_upper', 'upper', 'sentence_end', 'after_comma', 'sentence_start', 'char_count','avg_word', 'numerics', 'word_density']
        print(ret_res[col])
        return ret_res[col].values
    def fit(self, X, y=None, **fit_params):
        return self
    def get_params(self, deep=True):
        return {}
    
def text_transform(df):
    df["word_count"] = df["Sentence"].apply(lambda x: len(x.split()))
    df["has_upper"] = df["Sentence"].apply(lambda x: x.lower() != x).map({True:1,False:0})
    df['upper'] = df['Sentence'].apply(lambda x: len([x for x in x.split() if x.isupper()]))
    df["sentence_end"] = df["Sentence"].apply(lambda x: x.endswith(".")).map({True:1,False:0})
    df["after_comma"] = df["Sentence"].apply(lambda x: x.startswith(",")).map({True:1,False:0})
    df["sentence_start"] = df["Sentence"].apply(lambda x: "A" <= x[0] <= "Z").map({True:1,False:0})
    df["text"] = df["Sentence"].apply(lambda x: x.lower())
    df['char_count'] = df['Sentence'].str.len()
    def avg_word(sentence):
        words = sentence.split()
        return (sum(len(word) for word in words)/len(words))
    df['avg_word'] = df['Sentence'].apply(lambda x: avg_word(x))
    stop = stopwords.words('english')
    df['stopwords'] = df['Sentence'].apply(lambda x: len([x for x in x.split() if x in stop]))
    df['weighted_words'] = df["word_count"] - df['stopwords']
    df['numerics'] = df['Sentence'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
    df['word_density'] = df['char_count'] / (df['word_count']+1)
    df['punctuation_count'] = df['Sentence'].apply(lambda x: len("".join(_ for _ in x if _ in punctuation)))
    return df
	
def url_to_words(raw_text):
    raw_text=raw_text.strip()
    no_coms=re.sub(r'\.com','',raw_text)
    no_urls=re.sub('https?://www','',no_coms)
    no_urls1=re.sub('https?://','',no_urls)
    try:
        no_encoding=no_urls1.decode("utf-8-sig").replace(u"\ufffd", "?")
    except:
        no_encoding = no_urls1
    letters_only = re.sub("[^a-zA-Z0-9]", " ",no_encoding) 
    words = letters_only.split()                             
    stops = stopwords.words('english')
    meaningful_words = [w for w in words if not w in stops]
    return( " ".join( meaningful_words ))

def most_common(text):
    split_it = text.split() 
    most_occur = Counter(split_it).most_common(4)
    most_sent = 0
    for each in most_occur:
        most_sent += each[1]
    return most_sent

_obl = {"Insurance_CallCenter_LimitedOffers": 1, "Insurance_CallCenter_MarketManipulation": 2, "Insurance_CallCenter_PromotionalGifts": 3, "Insurance_CallCenter_ProductBenefits": 4, "Insurance_CallCenter_PolicyBreakdown": 5, "Insurance_CallCenter_Churning": 6,"Insurance_CallCenter_Mis-selling": 7, "Insurance_CallCenter_SalesObjectionBrushoffs": 8, "Insurance_CallCenter_TieInSales": 9, "Insurance_CallCenter_SalesCompetition": 10, "Insurance_CallCenter_FictitiousDocument": 11, "Insurance_CallCenter_BudgetObjection": 12, "Insurance_CallCenter_ProspectDecision": 13}
r_obl = {value: key for key, value in _obl.items()}
v_name = TfidfVectorizer(ngram_range=(1,2),stop_words="english", analyzer='word',max_features=10000)
vc_name = TfidfVectorizer(ngram_range=(1,3),stop_words="english", analyzer='char',max_features=10000)
vh_name = HashingVectorizer(ngram_range=(1,2),stop_words="english", analyzer='word')
vw_name = CountVectorizer(ngram_range=(1,2),stop_words="english", analyzer='word',max_features=10000)

OvR=OneVsRestClassifier(LogisticRegression(class_weight='balanced',random_state=1691))

def trainer():
    train=pd.read_excel('train_behavioral_petsure.xlsx')
    #train["label"] = train["label"].map(_obl)
    train['Sentence']=train['Sentence'].apply(url_to_words)
    pipe = Pipeline([
            ('feats', FeatureUnion([
                    ('extra', predictors()),
                    ('ngram_tf', v_name), 
                    ('ngram_count', vw_name),
                    ('ngram_char', vc_name)
                    ])),
        ('clf', OvR)
    ])
    X_train,X_val,y_train,y_val = train_test_split(train['Sentence'],train['Context'],test_size=0.2,random_state = 1994)
    pipe.fit(X_train,y_train)
    p=pipe.predict(X_val)
    print("f1_score :", f1_score(y_val,p,average='macro'))
    print("accuracy :", accuracy_score(y_val,p))
    pipe.fit(train['Sentence'], train['Context'])
    return pipe
	
def predictor(requestJson):
    pipe = joblib.load('behavioural.pkl')
    res_df = pd.DataFrame(data = [('','')], columns=['Sentence','Context'])
    # this line is needed while running in server system
    #test_json = json.loads(requestJson)
    
    for i in range(len(requestJson)):
        res_df.loc[i, ['Sentence']] = requestJson[i]['Sentence']
    res_df['Sentence']=res_df['Sentence'].apply(url_to_words)
    
    result = pd.DataFrame()   
    result['Sentence'] = res_df['Sentence']
    l1 = pipe.predict(res_df['Sentence'])
    l2 = pipe.predict_proba(res_df['Sentence'])
    result['Context'] = pd.Series(l1)
    result['Score'] = pd.Series([max(each) for each in l2])
    
    # Finding Avg Score
    context_avg_score = result.groupby(['Context'])['Score'].agg(['mean']).reset_index()
    context_avg_score.rename(columns={'mean': 'AvgScore'}, inplace=True)
    result = pd.merge(result, context_avg_score, how = 'left', left_on = ['Context'], right_on = ['Context'])
    #result.rename(columns={'PredictedContext': 'Context','Score':'Predicted_Score','Misconduct_Score': 'Score'}, inplace=True)

    type_list = []
    for each in set(result['Context']):
        conduct_type = dict()
        conduct_type['Type'] = each
        avg_score = 0
        sentences = []
        for i in range(result.shape[0]):
            if result['Context'][i] == str(each):
                sent = dict()
                sent['Sentence'] = result['Sentence'][i]
                sent['PredictedScore'] = result['Score'][i] * 100
                sent['MisconductScore'] = result['Score'][i] * 100
                avg_score = result['AvgScore'][i] 
                sentences.append(sent)
        conduct_type['Data'] = sentences
        conduct_type['AvgScore'] = avg_score * 100
        if avg_score >= 0.40:
            conduct_type['Status'] = 'Detected'
        else:
            conduct_type['Status'] = 'Not-Detected' 
        type_list.append(conduct_type)
    return type_list
